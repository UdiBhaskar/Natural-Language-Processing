{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "##Install spacy using pip (pip install spacy) then download \n",
    "#python -m spacy download en  #for english\n",
    "#python -m spacy download en_core_web_lg\n",
    "#python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@uday can't wait for the #nlp notes YAAAAAAY!!! #deeplearning https://udai.gitbook.io/practical-ml/\", 'That U.S.A. poster-print costs $12.40...', 'I am writing NLP basics.']\n"
     ]
    }
   ],
   "source": [
    "demo_sent1 = \"@uday can't wait for the #nlp notes YAAAAAAY!!! #deeplearning https://udai.gitbook.io/practical-ml/\"\n",
    "demo_sent2 = \"That U.S.A. poster-print costs $12.40...\"\n",
    "demo_sent3 = \"I am writing NLP basics.\"\n",
    "all_sents = [demo_sent1, demo_sent2, demo_sent3]\n",
    "print(all_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### White Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@uday', \"can't\", 'wait', 'for', 'the', '#nlp', 'notes', 'YAAAAAAY!!!', '#deeplearning', 'https://udai.gitbook.io/practical-ml/']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "['I', 'am', 'writing', 'NLP', 'basics.']\n"
     ]
    }
   ],
   "source": [
    "for sent in all_sents:\n",
    "    print(sent.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'uday', 'ca', \"n't\", 'wait', 'for', 'the', '#', 'nlp', 'notes', 'YAAAAAAY', '!', '!', '!', '#', 'deeplearning', 'https', ':', '//udai.gitbook.io/practical-ml/']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "['I', 'am', 'writing', 'NLP', 'basics', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "for sent in all_sents:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Regex Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'uday', 'can', \"'\", 't', 'wait', 'for', 'the', 'nlp', 'notes', 'YAAAAAAY', 'deeplearning', 'https', ':', 'udai', '.', 'gitbook', '.', 'io', 'practical-ml']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
      "['I', 'am', 'writing', 'NLP', 'basics', '.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "...     (?:[A-Z]\\.)+       # abbreviations, \n",
    "...   | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "...   | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, \n",
    "...   | \\.\\.\\.             # ellipsis\n",
    "...   | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    " '''\n",
    "for sent in all_sents:\n",
    "    print(nltk.regexp_tokenize(sent, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@uday', 'ca', \"n't\", 'wait', 'for', 'the', '#', 'nlp', 'notes', 'YAAAAAAY', '!', '!', '!', '#', 'deeplearning', 'https://udai.gitbook.io/practical-ml/']\n",
      "['That', 'U.S.A.', 'poster', '-', 'print', 'costs', '$', '12.40', '...']\n",
      "['I', 'am', 'writing', 'NLP', 'basics', '.']\n"
     ]
    }
   ],
   "source": [
    "##loading spaCy english module\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#printing\n",
    "for sent in all_sents:\n",
    "    print([token.text for token in nlp(sent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK workdnet lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "runner\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "##nltk lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('running'))\n",
    "print(lemmatizer.lemmatize('runner'))\n",
    "print(lemmatizer.lemmatize('runners'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "runner\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('running'))\n",
    "print(stemmer.stem('runner'))\n",
    "print(stemmer.stem('runners'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
