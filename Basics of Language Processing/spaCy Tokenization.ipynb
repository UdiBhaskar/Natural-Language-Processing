{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "##Install spacy using pip (pip install spacy) then download \n",
    "#python -m spacy download en  #for english\n",
    "#python -m spacy download de  #for German\n",
    "#python -m spacy download es  #for spanish\n",
    "#python -m spacy download pt  #for portuguese\n",
    "#python -m spacy download xx  #for multi-language\n",
    "#python -m spacy download en_core_web_lg\n",
    "#python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "at the center of the spaCy is the object containing the processing pipeline. you can get that class object from respective language package in the spaCy, for english spacy.lang.en.English is the object</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2174dd7fc88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an english instance\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#it will retuen an spaCy English object instance\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''i don't know whether the weather will improve or not.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i don't know whether the weather will improve or not.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A container for accessing the annotations. It let's you access information about the text in the structured way. \n",
    "#No infomation is lost. It behaves like normal python sequence.\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "i don't know whether\n"
     ]
    }
   ],
   "source": [
    "#You can check all the methods in https://spacy.io/api/doc\n",
    "print(doc[0])\n",
    "print(doc[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take a token\n",
    "token = doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "i\n",
      "True\n",
      "do\n",
      "6.4231944\n",
      "0.5209518\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "#you can check all the methods in https://spacy.io/api/token\n",
    "print(token.i) # get the index\n",
    "print(token.text) # to get text of that token\n",
    "print(token.is_alpha) #to check whether it is alpha chareters or not\n",
    "print(token.nbor()) #to get neighboring token\n",
    "print(token.vector_norm) # Norm of word vector (Glove Vector)\n",
    "print(token.similarity(token.nbor())) # similarity between words.\n",
    "print(token.lemma_) #lemmatized of word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><li><b>Lexemes - </b>These are just like tokens but context independent.</li></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemes = nlp.vocab['segmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7380086911887762763\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(lexemes.orth)# hash number \n",
    "print(lexemes.is_ascii)\n",
    "print(lexemes.has_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<li>spaCy encodes all the strings into hash values to save the memory.</li></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7380086911887762763\n",
      "segmentation\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab.strings['segmentation'])\n",
    "#you can also get in the reverse order\n",
    "print(nlp.vocab.strings[7380086911887762763])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><pre><b>Tokenization</b></pre></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'do', \"n't\", 'know', 'whether', 'the', 'weather', 'will', 'improve', 'or', 'not', '.']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<img src='https://spacy.io/tokenization-57e618bd79d933c4ccd308b5739062d6.svg'>\n",
    "\n",
    "You can customize your prefix, suffix, infix, Exceptions</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "prefix_re = re.compile(r'''^[[(\"']''')\n",
    "suffix_re = re.compile(r'''[])\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=simple_url_re.match)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(u\"hello-world.\")\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x18a4996f048>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an english instance\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#it will retuen an spaCy English object instance\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"hello-world.\")\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I like Camila Cabello. She lives in New York. She recently bought a red car.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x18af627de10>),\n",
       " ('parser', <spacy.pipeline.DependencyParser at 0x18af618e410>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x18af617ac50>)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we have nlp object and it contains below in that pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'Camila', 'Cabello', '.', 'She', 'lives', 'in', 'New', 'York', '.', 'She', 'recently', 'bought', 'a', 'red', 'car', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\"Camila Cabello\" is a person name but it came as two token. \"New York\" is a place name but for this also we got two tokens.  so we have to merge those.\n",
    "\n",
    "In spaCy, nlp object will uses some methods in the pipeline as shown in below. \n",
    "<img src=\"https://d33wubrfki0l68.cloudfront.net/16b2ccafeefd6d547171afa23f9ac62f159e353d/48b91/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg\"> \n",
    "so now we have to add another method after named entity recognizer to merge those entities. In spaCy, there is an internal function called \"merge_entities\" and that will do the same. </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x18af627de10>),\n",
       " ('parser', <spacy.pipeline.DependencyParser at 0x18af618e410>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x18af617ac50>),\n",
       " ('merge_entities', <function spacy.pipeline.merge_entities>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipe\n",
    "merge_entities = nlp.create_pipe(\"merge_entities\")\n",
    "# adding it to nlp object\n",
    "nlp.add_pipe(merge_entities)\n",
    "#printing all methods in the pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'Camila Cabello', '.', 'She', 'lives', 'in', 'New York', '.', 'She', 'recently', 'bought', 'a', 'red', 'car', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to write a function, you can write like this, \n",
    "# it has to take doc object as input and has to give doc object as output.\n",
    "def merge_entities(doc):\n",
    "    \"\"\"Merge entities into a single token.\"\"\"\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            attrs = {\"tag\": ent.root.tag, \"dep\": ent.root.dep, \"ent_type\": ent.label}\n",
    "            retokenizer.merge(ent, attrs=attrs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre> \"a red car\" is one word ( a noun chunk ) so we have to merge these type of words as well. In spaCy, there is an internal function called \"merge_noun_chunks\" and that will do the same. </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x18af627de10>),\n",
       " ('parser', <spacy.pipeline.DependencyParser at 0x18af618e410>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x18af617ac50>),\n",
       " ('merge_entities', <function spacy.pipeline.merge_entities>),\n",
       " ('merge_noun_chunks', <function spacy.pipeline.merge_noun_chunks>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipe\n",
    "merge_noun_chunk = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "# adding it to nlp object\n",
    "nlp.add_pipe(merge_noun_chunk)\n",
    "#printing all methods in the pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'Camila Cabello', '.', 'She', 'lives', 'in', 'New York', '.', 'She', 'recently', 'bought', 'a red car', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><pre><b>Lemmatization</b></pre></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', '-PRON-'), ('like', 'like'), ('Camila Cabello', 'camila cabello'), ('.', '.'), ('She', '-PRON-'), ('lives', 'live'), ('in', 'in'), ('New York', 'new york'), ('.', '.'), ('She', '-PRON-'), ('recently', 'recently'), ('bought', 'buy'), ('a red car', 'a'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#you can direclt get lemmatized words from doc\n",
    "print([(token.text,token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><pre><b>POS Tagging</b></pre></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRON'), ('like', 'VERB'), ('Camila Cabello', 'PROPN'), ('.', 'PUNCT'), ('She', 'PRON'), ('lives', 'VERB'), ('in', 'ADP'), ('New York', 'PROPN'), ('.', 'PUNCT'), ('She', 'PRON'), ('recently', 'ADV'), ('bought', 'VERB'), ('a red car', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "print([(token.text,token.pos_) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><pre><b>Entity Detection</b></pre></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Camila Cabello, New York)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Camila Cabello, 'PERSON'), (New York, 'GPE')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ent, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">I like \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Camila Cabello\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ". She lives in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". She recently bought a red car.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style = \"ent\",jupyter = True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
