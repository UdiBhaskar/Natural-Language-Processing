{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6c-oVpYCylih"
   },
   "source": [
    "# Word2Vec - Tensorflow, Skip-Gram, NCS Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nl-WBKvhKssG"
   },
   "outputs": [],
   "source": [
    "##imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "##\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, Dot, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7AlP8jTbMNNu"
   },
   "outputs": [],
   "source": [
    "##getting data from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/kernels?sortBy=voteCount&group=everyone&pageSize=20&datasetId=134715\n",
    "data_imdb = pd.read_csv(r'/content/drive/My Drive/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "t97fq-rRMO29",
    "outputId": "ef70b402-4be8-49ae-f02b-e08d7c0407a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#info\n",
    "data_imdb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nwfEfTW4MRqp"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Cleans the text data'''\n",
    "    ##remove html tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-8aX8FYMRtG"
   },
   "outputs": [],
   "source": [
    "data_imdb['review'] = data_imdb['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "0DiC8X84MRvN",
    "outputId": "43f87427-a973-45cf-ca2f-a0c17677bcd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "v4acI6MHi-3v",
    "outputId": "e1abff70-4689-40b3-abe6-106560de7473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3YGEAvEMRxe"
   },
   "outputs": [],
   "source": [
    "##getting sentence wise data\n",
    "list_sents = [nltk.word_tokenize(sent) for sent_tok in data_imdb.review for sent in nltk.sent_tokenize(sent_tok)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4n9D2SbBWzMD"
   },
   "outputs": [],
   "source": [
    "##to use tf.keras.preprocessing.sequence.skipgrams, we have to encode our sentence to numbers. so used Tokenizer class\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list_sents)\n",
    "seq_texts = tokenizer.texts_to_sequences(list_sents) ##list of list+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBX2thhdMfyM"
   },
   "outputs": [],
   "source": [
    "class word2vecNCS(Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, num_sampled, **kwargs):\n",
    "        '''NCS Word2Vec\n",
    "        vocab_size: Size of vocabulary you have\n",
    "        embed_size: Embedding size needed\n",
    "        num_sampled: No of negative sampled to generate'''\n",
    "        super(word2vecNCS, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_sampled = num_sampled\n",
    "        ##embedding layer\n",
    "        self.embed_layer = Embedding(input_dim=vocab_size, output_dim=embed_size,embeddings_initializer=tf.keras.initializers.RandomUniform(seed=32))\n",
    "        ##reshing layer\n",
    "        self.reshape_layer = Reshape((self.embed_size,))\n",
    "    def build(self, input_shape):\n",
    "        ##weights needed for nce loss\n",
    "        self.nce_weight = self.add_weight(shape=(self.vocab_size, self.embed_size),\n",
    "                             initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev= (1/self.embed_size**0.5)),\n",
    "                             trainable=True, name=\"nce_weight\")\n",
    "        #biases needed nce loss\n",
    "        self.nce_bias = self.add_weight(shape=(self.vocab_size), initializer=\"zeros\", trainable=True, name=\"nce_bias\")\n",
    "\n",
    "    def call(self, input_center_word, input_context_word):\n",
    "        '''\n",
    "        input_center_word: center word\n",
    "        input_context_word: context word''' \n",
    "        ##giving center word and getting the embedding\n",
    "        embedd_out = self.embed_layer(input_center_word)\n",
    "        ##rehaping \n",
    "        embedd_out = self.reshape_layer(embedd_out)\n",
    "        ##calculating nce loss\n",
    "        nce_loss = tf.reduce_sum(tf.nn.nce_loss(weights=self.nce_weight, \n",
    "                                  biases=self.nce_bias, \n",
    "                                  labels=input_context_word, \n",
    "                                  inputs=embedd_out, \n",
    "                                  num_sampled=self.num_sampled, \n",
    "                                  num_classes=self.vocab_size))\n",
    "        return nce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAgAnc6ZMf06"
   },
   "outputs": [],
   "source": [
    "def generate_sgns():\n",
    "    for seq in seq_texts:\n",
    "        generated_samples, labels = tf.keras.preprocessing.sequence.skipgrams(sequence=seq, \n",
    "                                                                      vocabulary_size=len(tokenizer.word_index)+1, \n",
    "                                                                      window_size=2, negative_samples=0)\n",
    "        length_samples = len(generated_samples)\n",
    "        for i in range(length_samples):\n",
    "            yield [generated_samples[i][0]], [generated_samples[i][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BY8tN5eMf3O"
   },
   "outputs": [],
   "source": [
    "##creating the tf dataset\n",
    "tfdataset_gen = tf.data.Dataset.from_generator(generate_sgns, output_types=(tf.int64, tf.int64))\n",
    "tfdataset_gen = tfdataset_gen.repeat().batch(1024).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874
    },
    "colab_type": "code",
    "id": "zADMiM3mq0ST",
    "outputId": "99f6799e-803a-4373-cc06-bdaedad71c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 100 iterations, Loss: 178544.187500\n",
      "Done 200 iterations, Loss: 175835.203125\n",
      "Saving checkpoint for iteration 201 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-1\n",
      "Done 300 iterations, Loss: 169735.578125\n",
      "Done 400 iterations, Loss: 164666.984375\n",
      "Saving checkpoint for iteration 401 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-2\n",
      "Done 500 iterations, Loss: 162460.453125\n",
      "Done 600 iterations, Loss: 159760.906250\n",
      "Saving checkpoint for iteration 601 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-3\n",
      "Done 700 iterations, Loss: 151500.765625\n",
      "Done 800 iterations, Loss: 150536.187500\n",
      "Saving checkpoint for iteration 801 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-4\n",
      "Done 900 iterations, Loss: 149034.468750\n",
      "Done 1000 iterations, Loss: 150517.093750\n",
      "Saving checkpoint for iteration 1001 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-5\n",
      "Done 1100 iterations, Loss: 142637.484375\n",
      "Done 1200 iterations, Loss: 139848.828125\n",
      "Saving checkpoint for iteration 1201 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-6\n",
      "Done 1300 iterations, Loss: 135867.515625\n",
      "Done 1400 iterations, Loss: 141921.812500\n",
      "Saving checkpoint for iteration 1401 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-7\n",
      "Done 1500 iterations, Loss: 131798.937500\n",
      "Done 1600 iterations, Loss: 127753.531250\n",
      "Saving checkpoint for iteration 1601 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-8\n",
      "Done 1700 iterations, Loss: 128081.742188\n",
      "Done 1800 iterations, Loss: 128337.757812\n",
      "Saving checkpoint for iteration 1801 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-9\n",
      "Done 1900 iterations, Loss: 126370.398438\n",
      "Done 2000 iterations, Loss: 126191.296875\n",
      "Saving checkpoint for iteration 2001 at /content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3bab8235a2ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgncs_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mname_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgncs_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/histogram/summary_v2.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(name, data, step, buckets, description)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mhistogram_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         )\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistogram_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/histogram/summary_v2.py\u001b[0m in \u001b[0;36mhistogram_summary\u001b[0;34m(data, buckets, histogram_metadata, step)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummary_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(tag, tensor, step, metadata, name)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     op = smart_cond.smart_cond(\n\u001b[0;32m--> 675\u001b[0;31m         _should_record_summaries_v2(), record, _nothing, name=\"summary_cond\")\n\u001b[0m\u001b[1;32m    676\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mrecord\u001b[0;34m()\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0;31m# Note the identity to move the tensor to the CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         summary_tensor = tensor() if callable(tensor) else array_ops.identity(\n\u001b[0m\u001b[1;32m    663\u001b[0m             tensor)\n\u001b[1;32m    664\u001b[0m         write_summary_op = gen_summary_ops.write_summary(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorboard/util/lazy_tensor_creator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CALL_IN_PROGRESS_SENTINEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/histogram/summary_v2.py\u001b[0m in \u001b[0;36mlazy_tensor\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mlazy_tensor_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLazyTensorCreator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mlazy_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_buckets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             return tf.summary.write(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/histogram/summary_v2.py\u001b[0m in \u001b[0;36m_buckets\u001b[0;34m(data, bucket_count)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_singular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen_singular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen_nonsingular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen_empty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen_nonempty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond_for_tf_v2\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m   \"\"\"\n\u001b[0;32m-> 1392\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfalse_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_UnpackIfSingleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/histogram/summary_v2.py\u001b[0m in \u001b[0;36mwhen_nonempty\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m                 )\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_singular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen_singular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen_nonsingular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen_empty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen_nonempty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond_for_tf_v2\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m   \"\"\"\n\u001b[0;32m-> 1392\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfalse_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_UnpackIfSingleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/histogram/summary_v2.py\u001b[0m in \u001b[0;36mwhen_nonsingular\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mone_hots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclamped_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 bucket_counts = tf.cast(\n\u001b[0;32m--> 148\u001b[0;31m                     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mone_hots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0;32m-> 1742\u001b[0;31m                               _ReductionDims(input_tensor, axis))\n\u001b[0m\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   1751\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   1752\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  10155\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m  10156\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10157\u001b[0;31m         input, axis, \"keep_dims\", keep_dims)\n\u001b[0m\u001b[1;32m  10158\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10159\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##training\n",
    "\n",
    "##optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "\n",
    "sgncs_w2v = word2vecNCS(len(tokenizer.word_index)+1, 100, 32)\n",
    "\n",
    "##train step function to train\n",
    "@tf.function\n",
    "def train_step(input_center, input_context):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward propagation\n",
    "        loss = sgncs_w2v(input_center, input_context)\n",
    "\n",
    "    #getting gradients\n",
    "    gradients = tape.gradient(loss, sgncs_w2v.trainable_variables)\n",
    "    #applying gradients\n",
    "    optimizer.apply_gradients(zip(gradients, sgncs_w2v.trainable_variables))\n",
    "\n",
    "    return loss, gradients\n",
    "\n",
    "##number of epochs\n",
    "no_iterations=10000\n",
    "\n",
    "##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "#tensorboard file writers\n",
    "wtrain = tf.summary.create_file_writer(logdir='/content/drive/My Drive/word2vec_logs/logs/w2vncs/train')\n",
    "\n",
    "##check point to save\n",
    "checkpoint_path = \"/content/drive/My Drive/word2vec_logs/checkpoints/w2vNCS/train\"\n",
    "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=sgncs_w2v)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "\n",
    "counter = 0\n",
    "#training anf validating\n",
    "for in_center, in_context in tfdataset_gen:\n",
    "    #train step\n",
    "    loss_, gradients = train_step(in_center, in_context)\n",
    "    #adding loss to train loss\n",
    "    train_loss(loss_)\n",
    "\n",
    "    counter = counter + 1\n",
    "         \n",
    "    ##tensorboard \n",
    "    with tf.name_scope('per_step_training'):\n",
    "        with wtrain.as_default():\n",
    "            tf.summary.scalar(\"batch_loss\", loss_, step=counter)\n",
    "    with tf.name_scope(\"per_batch_gradients\"):\n",
    "        with wtrain.as_default():\n",
    "            for i in range(len(sgncs_w2v.trainable_variables)):\n",
    "                name_temp = sgncs_w2v.trainable_variables[i].name\n",
    "                tf.summary.histogram(name_temp, gradients[i], step=counter)\n",
    "    \n",
    "    if counter%100 == 0:\n",
    "        #printing\n",
    "        template = '''Done {} iterations, Loss: {:0.6f}'''\n",
    "    \n",
    "        print(template.format(counter, train_loss.result()))\n",
    "\n",
    "        if counter%200 == 0:\n",
    "            ckpt_save_path  = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for iteration {} at {}'.format(counter+1, ckpt_save_path))\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "    if counter > no_iterations :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Sy129w7xttX4",
    "outputId": "39fab1aa-ee7c-43c3-99ee-dbcde3476c28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-8',\n",
       " '/content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-9',\n",
       " '/content/drive/My Drive/word2vec/checkpoints/w2vNCS/train/ckpt-10']"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##you can load from below checkpoints\n",
    "ckpt_manager.checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "pC-qiYibxHPF",
    "outputId": "e31c6e5a-b34b-4c86-8223-0f72f033a959"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.3952752e-02, -3.9126575e-02,  4.7340643e-02, ...,\n",
       "        -1.5561890e-02, -8.4457621e-03, -3.8311258e-03],\n",
       "       [ 1.2673739e-02,  2.6604220e-01, -1.2200137e-01, ...,\n",
       "        -2.6649845e-01,  4.3133581e-01,  1.6652634e+00],\n",
       "       [ 5.0035398e-02,  2.2489849e-01,  1.3173567e-02, ...,\n",
       "         5.7263959e-02,  2.8345990e-01,  1.7488687e+00],\n",
       "       ...,\n",
       "       [ 8.9006796e-03, -1.7985929e-02,  2.8799288e-03, ...,\n",
       "         7.4115507e-03, -1.2984991e-02,  1.3159607e-02],\n",
       "       [ 1.2338273e-03,  1.1289597e-02, -1.5265692e-02, ...,\n",
       "        -1.8576480e-02,  1.1550784e-02, -1.6645409e-02],\n",
       "       [-3.9734341e-02,  2.7531121e-02, -3.8868021e-02, ...,\n",
       "        -3.4919988e-02,  2.7181890e-02,  2.0708870e-02]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgncs_w2v.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwlY2UuBqE7N"
   },
   "outputs": [],
   "source": [
    "#3getting the word vectors\n",
    "word_vectors = sgncs_w2v.get_weights()[0]\n",
    "##convering into dict\n",
    "word_vectors_dict = {}\n",
    "for word, number in tokenizer.word_index.items():\n",
    "    word_vectors_dict[word] = word_vectors[number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "xhaCvu0pqKKA",
    "outputId": "80bac86f-fb2f-417e-b2fb-f46d7947455b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02759881,  0.375678  ,  0.00394442,  0.03537954, -0.3508165 ,\n",
       "       -0.3843107 , -1.902659  , -0.03924064,  0.03567823,  0.22394422,\n",
       "        1.3254056 , -0.2738524 , -0.10814386, -0.16480844,  1.5734187 ,\n",
       "        0.24689345,  0.1799947 , -0.31367883, -0.03789733, -0.2153853 ,\n",
       "        0.23517315, -0.43801263, -1.3624109 , -1.4466581 ,  1.2364206 ,\n",
       "        0.16642871, -0.05696939, -1.5534589 ,  0.08557777,  0.19669291,\n",
       "       -0.37249157, -0.21300569, -0.03838867,  0.08273836, -0.10653205,\n",
       "        0.06428144, -0.15504088, -0.17317666,  0.17255655,  0.2918513 ,\n",
       "        1.3766997 ,  0.40330094,  0.46494156,  0.80065376, -0.8771551 ,\n",
       "       -0.04986344, -0.8916342 ,  0.03179355,  1.5216345 ,  0.18060522,\n",
       "        0.06709374, -0.0697518 , -0.11124301,  0.18427111, -1.3699241 ,\n",
       "        0.10965209, -1.0532825 ,  0.37451154, -0.08506859, -0.0325211 ,\n",
       "        0.298475  ,  0.15674552, -0.06431884, -0.260983  , -0.02124155,\n",
       "        0.07537135, -0.42176956, -0.22111684,  1.740518  , -0.26797977,\n",
       "       -0.37353724, -0.06697398, -0.1167245 ,  0.05020875, -0.39264953,\n",
       "        0.20606215,  1.4921551 , -0.14778323, -1.4355932 , -0.13270906,\n",
       "        0.25163716,  1.7350756 ,  0.34582663, -0.31447065,  0.37216064,\n",
       "        0.29989046, -0.15839949, -0.10046904,  0.02564561,  0.38057965,\n",
       "       -0.24561724, -0.15449129, -0.12380957, -1.4958894 , -0.00739242,\n",
       "       -0.15167639, -0.34693414, -0.27081248,  0.23397827,  1.4521714 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##word vector for movies\n",
    "word_vectors_dict['movies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDdMI14xqMI8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIsJONWnqO1_"
   },
   "outputs": [],
   "source": [
    "##https://stackoverflow.com/a/54809609/10899915\n",
    "def save_word2vec_format_dict(fname, vocab, vectors, binary=True, total_vec=None):\n",
    "  \"\"\"Store the input-hidden weight matrix in the same format used by the original\n",
    "    C word2vec-tool, for compatibility.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        The file path used to save the vectors in.\n",
    "    vocab : dict\n",
    "        The vocabulary of words.\n",
    "    vectors : numpy.array\n",
    "        The vectors to be stored.\n",
    "    binary : bool, optional\n",
    "        If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
    "    total_vec : int, optional\n",
    "        Explicitly specify total number of vectors\n",
    "        (in case word vectors are appended with document vectors afterwards).\n",
    "    \"\"\"\n",
    "  if not (vocab or vectors):\n",
    "      raise RuntimeError(\"no input\")\n",
    "  if total_vec is None:\n",
    "      total_vec = len(vocab)\n",
    "  vector_size = vectors.shape[1]\n",
    "  assert (len(vocab), vector_size) == vectors.shape\n",
    "  with utils.smart_open(fname, 'wb') as fout:\n",
    "      print(total_vec, vector_size)\n",
    "      fout.write(utils.to_utf8(\"%s %s\\n\" % (total_vec, vector_size)))\n",
    "      # store in sorted order: most frequent words at the top\n",
    "      for word, row in vocab.items():\n",
    "          if binary:\n",
    "              row = row.astype(REAL)\n",
    "              fout.write(utils.to_utf8(word) + b\" \" + row.tostring())\n",
    "          else:\n",
    "              fout.write(utils.to_utf8(\"%s %s\\n\" % (word, ' '.join(repr(val) for val in row))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R1aMzvD2qPpy",
    "outputId": "94f0045f-a61f-4b5e-acc2-5fb30d5807b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206705 100\n"
     ]
    }
   ],
   "source": [
    "##saving the file\n",
    "from numpy import zeros, dtype, float32 as REAL, ascontiguousarray, fromstring\n",
    "from gensim import utils\n",
    "\n",
    "model_gensim = gensim.models.keyedvectors.Word2VecKeyedVectors(vector_size=50)\n",
    "model_gensim.vocab = word_vectors_dict\n",
    "model_gensim.vectors = np.array(list(word_vectors_dict.values()))\n",
    "save_word2vec_format_dict(binary=True, fname='w2vncs.bin', total_vec=len(word_vectors_dict), vocab=model_gensim.vocab, vectors=model_gensim.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZYvqMzmqRt2"
   },
   "outputs": [],
   "source": [
    "##loading model\n",
    "model_gensim = gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format('w2vncs.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "i7tDXXVqqTQG",
    "outputId": "35886bb5-e456-4a1f-befe-62670838e3e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02759881,  0.375678  ,  0.00394442,  0.03537954, -0.3508165 ,\n",
       "       -0.3843107 , -1.902659  , -0.03924064,  0.03567823,  0.22394422,\n",
       "        1.3254056 , -0.2738524 , -0.10814386, -0.16480844,  1.5734187 ,\n",
       "        0.24689345,  0.1799947 , -0.31367883, -0.03789733, -0.2153853 ,\n",
       "        0.23517315, -0.43801263, -1.3624109 , -1.4466581 ,  1.2364206 ,\n",
       "        0.16642871, -0.05696939, -1.5534589 ,  0.08557777,  0.19669291,\n",
       "       -0.37249157, -0.21300569, -0.03838867,  0.08273836, -0.10653205,\n",
       "        0.06428144, -0.15504088, -0.17317666,  0.17255655,  0.2918513 ,\n",
       "        1.3766997 ,  0.40330094,  0.46494156,  0.80065376, -0.8771551 ,\n",
       "       -0.04986344, -0.8916342 ,  0.03179355,  1.5216345 ,  0.18060522,\n",
       "        0.06709374, -0.0697518 , -0.11124301,  0.18427111, -1.3699241 ,\n",
       "        0.10965209, -1.0532825 ,  0.37451154, -0.08506859, -0.0325211 ,\n",
       "        0.298475  ,  0.15674552, -0.06431884, -0.260983  , -0.02124155,\n",
       "        0.07537135, -0.42176956, -0.22111684,  1.740518  , -0.26797977,\n",
       "       -0.37353724, -0.06697398, -0.1167245 ,  0.05020875, -0.39264953,\n",
       "        0.20606215,  1.4921551 , -0.14778323, -1.4355932 , -0.13270906,\n",
       "        0.25163716,  1.7350756 ,  0.34582663, -0.31447065,  0.37216064,\n",
       "        0.29989046, -0.15839949, -0.10046904,  0.02564561,  0.38057965,\n",
       "       -0.24561724, -0.15449129, -0.12380957, -1.4958894 , -0.00739242,\n",
       "       -0.15167639, -0.34693414, -0.27081248,  0.23397827,  1.4521714 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##word vectors\n",
    "model_gensim.wv['movies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "zvbMU6DQqWIV",
    "outputId": "ebbb30b5-8507-4b6f-c564-1e6b06578511"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('on', 0.9762333631515503),\n",
       " ('one', 0.9754350185394287),\n",
       " ('is', 0.9748979806900024),\n",
       " ('about', 0.9745714664459229),\n",
       " ('for', 0.9742895364761353),\n",
       " ('like', 0.9740601778030396),\n",
       " ('film', 0.9738096594810486),\n",
       " ('with', 0.9723396301269531),\n",
       " ('in', 0.9720105528831482),\n",
       " ('so', 0.9718752503395081)]"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gensim.wv.most_similar(positive='movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUsyMjgszCTm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NCE Word2ve.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
